{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0af561a3-b0c2-479b-ab52-4c0f6a1c6792",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libs\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import r2_score,mean_absolute_error,mean_squared_error\n",
    "from sklearn.linear_model import LogisticRegression,Lasso,LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import metrics\n",
    "from scipy.stats import zscore\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor,GradientBoostingRegressor\n",
    "from sklearn.preprocessing import LabelEncoder,StandardScaler\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression,Lasso,Ridge,ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cda8e9a5-6b25-4cfa-88b5-ddd44fdfe6b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "DataSetNotFoundError",
     "evalue": "DataSet 'election_cleaned_dataset' not found in the catalog - did you mean one of these instead: election_normalized_dataset, election_databse, election_without_outliers_dataset",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDataSetNotFoundError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/l5/26f7vmhn489dq9k00762x2bw0000gn/T/ipykernel_7267/1229634437.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcatalog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'election_cleaned_dataset'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/kedro/io/data_catalog.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, name, version)\u001b[0m\n\u001b[1;32m    334\u001b[0m         \"\"\"\n\u001b[1;32m    335\u001b[0m         \u001b[0mload_version\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mload_version\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         self._logger.info(\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/kedro/io/data_catalog.py\u001b[0m in \u001b[0;36m_get_dataset\u001b[0;34m(self, data_set_name, version)\u001b[0m\n\u001b[1;32m    293\u001b[0m                 \u001b[0merror_msg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\" - did you mean one of these instead: {suggestions}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mDataSetNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0mdata_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_sets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_set_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDataSetNotFoundError\u001b[0m: DataSet 'election_cleaned_dataset' not found in the catalog - did you mean one of these instead: election_normalized_dataset, election_databse, election_without_outliers_dataset"
     ]
    }
   ],
   "source": [
    "df = catalog.load('election_cleaned_dataset')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6075e3f5-a608-4d18-847d-d3afe707d0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizando os dados\n",
    "cols_normalizado = ['territoryName','totalMandates', 'numParishes', 'numParishesApproved', 'blankVotes', 'nullVotes', 'subscribedVoters', 'totalVoters', 'pre.blankVotes', 'pre.nullVotes', 'pre.subscribedVoters', 'pre.totalVoters', 'Party', 'Mandates', 'Votes','Hondt', 'FinalMandates']\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "norm = MinMaxScaler()\n",
    "\n",
    "df[cols_normalizado] = norm.fit_transform(df[cols_normalizado])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e0bbae-bf90-4ca8-adaa-3c0effc60398",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split train and test \n",
    "\n",
    "X = df.drop(columns=['FinalMandates'])\n",
    "y = df['FinalMandates']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8627de31-195d-4f8c-931a-7cc15e867e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df, y, test_size = 0.3, random_state = 9999, stratify = y\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81e2f8e-d57d-4a4a-bf75-2267350aa085",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194b2139-33d2-4de5-921c-c77c31ce8add",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "classifier = tree.DecisionTreeClassifier()\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45c72c4-2d9e-4f90-a94b-0be0d3806bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "score = accuracy_score(y_test, y_pred)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f6a89b-880a-4636-9f03-7dc02c837b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Funcao fit model\n",
    "\n",
    "\n",
    "def func_split_data(df, test_size = 0.3, random_state = 9999):\n",
    "    \n",
    "    \"\"\"Splits data into train and test datasets\"\"\"\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df, y, test_size = 0.3, random_state = 9999, stratify = y\n",
    "    )\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def func_fit_model(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    \"\"\"Fits a classifier model using a training dataset\"\"\"\n",
    "    \n",
    "    classifier = tree.DecisionTreeClassifier()\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    #Predicts test set \n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    #Reports Score\n",
    "    score = accuracy_score(y_test, y_pred)\n",
    "    print(score)\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50862c0b-1fb1-4fe9-816d-d3e19a5fa92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test da funcao\n",
    "\n",
    "X_train, X_test, y_train, y_test = func_split_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4125e7-c940-4343-9da5-a5d7fc4725c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "func_fit_model(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a6b6e4-6105-41e8-874c-533c4a619a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining a function to find model score,r2 score for the given dataset\n",
    "model=[LinearRegression(),DecisionTreeRegressor(),RandomForestRegressor()]\n",
    "\n",
    "for m in model:\n",
    "    m.fit(X_train, y_train)\n",
    "    score = m.score(X_train, y_train)\n",
    "    predm=m.predict(X_test)\n",
    "    print('Score of',m,'is:',score)\n",
    "    print('MAE:',mean_absolute_error(y_test,predm))\n",
    "    print('MSE:',mean_squared_error(y_test,predm))\n",
    "    print('RMSE:',np.sqrt(mean_squared_error(y_test,predm)))\n",
    "    print('R2 score:',r2_score(y_test,predm))\n",
    "    print('*'*100)\n",
    "    print('\\n') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e736dd47-c40f-412e-b098-62ece0412f7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kedro (project_ds_kedro)",
   "language": "python",
   "name": "kedro_project_ds_kedro"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
